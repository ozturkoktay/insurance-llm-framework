"""
Human evaluation module for insurance domain LLM outputs.

This module provides utilities for collecting and analyzing human feedback
on LLM-generated content for insurance tasks.
"""

import logging
import json
import csv
import os
import datetime
from typing import Dict, List, Optional, Any, Union
from dataclasses import dataclass, asdict, field
from pathlib import Path

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# Default path for storing evaluation results
DEFAULT_EVALUATIONS_DIR = os.path.join(
    os.path.dirname(__file__), "evaluations")


@dataclass
class HumanEvaluationCriteria:
    """Class representing a single human evaluation criterion."""

    name: str
    description: str
    min_score: int
    max_score: int
    rubric: Dict[int, str]


@dataclass
class HumanEvaluationForm:
    """Class representing a human evaluation form with multiple criteria."""

    id: str
    title: str
    description: str
    criteria: List[HumanEvaluationCriteria]

    @classmethod
    def create_default_form(cls, task_type: str) -> 'HumanEvaluationForm':
        """
        Create a default evaluation form for a specific task type.

        Args:
            task_type: Type of insurance task

        Returns:
            A default evaluation form
        """
        if task_type == "policy_summary":
            return cls(
                id=f"policy_summary_eval_{datetime.datetime.now().strftime('%Y%m%d')}",
                title="Policy Summary Evaluation",
                description="Evaluate the quality of the policy summary generated by the LLM.",
                criteria=[
                    HumanEvaluationCriteria(
                        name="accuracy",
                        description="Does the summary accurately reflect the content of the original policy?",
                        min_score=1,
                        max_score=5,
                        rubric={
                            1: "Very inaccurate, contains major factual errors",
                            2: "Somewhat inaccurate, contains minor factual errors",
                            3: "Moderately accurate, with some omissions or misrepresentations",
                            4: "Mostly accurate, with minimal errors or omissions",
                            5: "Completely accurate, no errors or omissions"
                        }
                    ),
                    HumanEvaluationCriteria(
                        name="completeness",
                        description="Does the summary include all key information from the policy?",
                        min_score=1,
                        max_score=5,
                        rubric={
                            1: "Very incomplete, missing most key information",
                            2: "Somewhat incomplete, missing several key pieces of information",
                            3: "Moderately complete, covers main points but misses some details",
                            4: "Mostly complete, covers nearly all key information",
                            5: "Completely comprehensive, covers all key information"
                        }
                    ),
                    HumanEvaluationCriteria(
                        name="clarity",
                        description="Is the summary clear and easy to understand?",
                        min_score=1,
                        max_score=5,
                        rubric={
                            1: "Very unclear, difficult to understand",
                            2: "Somewhat unclear, requires significant effort to understand",
                            3: "Moderately clear, some parts require clarification",
                            4: "Mostly clear, minor issues with clarity",
                            5: "Perfectly clear, very easy to understand"
                        }
                    ),
                    HumanEvaluationCriteria(
                        name="conciseness",
                        description="Is the summary appropriately concise without omitting key information?",
                        min_score=1,
                        max_score=5,
                        rubric={
                            1: "Very verbose or too short to be useful",
                            2: "Somewhat lengthy or missing substantial content",
                            3: "Moderately concise, but could be improved",
                            4: "Good balance of conciseness and completeness",
                            5: "Perfect length, captures all key information efficiently"
                        }
                    ),
                ]
            )
        elif task_type == "claim_response":
            return cls(
                id=f"claim_response_eval_{datetime.datetime.now().strftime('%Y%m%d')}",
                title="Claim Response Evaluation",
                description="Evaluate the quality of the claim response generated by the LLM.",
                criteria=[
                    HumanEvaluationCriteria(
                        name="accuracy",
                        description="Is the response accurate regarding policy details and claim information?",
                        min_score=1,
                        max_score=5,
                        rubric={
                            1: "Very inaccurate, contains major factual errors",
                            2: "Somewhat inaccurate, contains minor factual errors",
                            3: "Moderately accurate, with some omissions or misrepresentations",
                            4: "Mostly accurate, with minimal errors or omissions",
                            5: "Completely accurate, no errors or omissions"
                        }
                    ),
                    HumanEvaluationCriteria(
                        name="professionalism",
                        description="Is the response professional and appropriate for a customer?",
                        min_score=1,
                        max_score=5,
                        rubric={
                            1: "Very unprofessional, inappropriate tone or language",
                            2: "Somewhat unprofessional, issues with tone or formality",
                            3: "Moderately professional, some room for improvement",
                            4: "Mostly professional, minor issues with tone",
                            5: "Perfectly professional, appropriate tone and language"
                        }
                    ),
                    HumanEvaluationCriteria(
                        name="helpfulness",
                        description="Is the response helpful and informative for the customer?",
                        min_score=1,
                        max_score=5,
                        rubric={
                            1: "Not helpful, doesn't address the customer's needs",
                            2: "Minimally helpful, addresses few of the customer's needs",
                            3: "Moderately helpful, addresses some but not all needs",
                            4: "Very helpful, addresses most of the customer's needs",
                            5: "Extremely helpful, fully addresses all customer needs"
                        }
                    ),
                    HumanEvaluationCriteria(
                        name="compliance",
                        description="Does the response comply with insurance regulations and best practices?",
                        min_score=1,
                        max_score=5,
                        rubric={
                            1: "Major compliance issues, could lead to legal problems",
                            2: "Some compliance concerns that should be addressed",
                            3: "Generally compliant, but some areas could be improved",
                            4: "Mostly compliant, minor issues or improvements possible",
                            5: "Fully compliant with all regulations and best practices"
                        }
                    ),
                ]
            )
        elif task_type == "customer_communication":
            return cls(
                id=f"customer_comm_eval_{datetime.datetime.now().strftime('%Y%m%d')}",
                title="Customer Communication Evaluation",
                description="Evaluate the quality of the customer communication generated by the LLM.",
                criteria=[
                    HumanEvaluationCriteria(
                        name="clarity",
                        description="Is the communication clear and easy to understand?",
                        min_score=1,
                        max_score=5,
                        rubric={
                            1: "Very unclear, difficult to understand",
                            2: "Somewhat unclear, requires significant effort to understand",
                            3: "Moderately clear, some parts require clarification",
                            4: "Mostly clear, minor issues with clarity",
                            5: "Perfectly clear, very easy to understand"
                        }
                    ),
                    HumanEvaluationCriteria(
                        name="empathy",
                        description="Does the communication show appropriate empathy and understanding?",
                        min_score=1,
                        max_score=5,
                        rubric={
                            1: "No empathy, cold or insensitive",
                            2: "Minimal empathy, mostly factual and detached",
                            3: "Some empathy, but could be more personalized",
                            4: "Good empathy, shows understanding of customer situation",
                            5: "Excellent empathy, perfectly balances understanding and professionalism"
                        }
                    ),
                    HumanEvaluationCriteria(
                        name="relevance",
                        description="Is the communication relevant to the customer's inquiry or situation?",
                        min_score=1,
                        max_score=5,
                        rubric={
                            1: "Not relevant, doesn't address the customer's inquiry",
                            2: "Minimally relevant, addresses few aspects of the inquiry",
                            3: "Moderately relevant, addresses main points but misses some",
                            4: "Mostly relevant, addresses nearly all aspects of the inquiry",
                            5: "Perfectly relevant, fully addresses the customer's inquiry"
                        }
                    ),
                    HumanEvaluationCriteria(
                        name="actionability",
                        description="Does the communication provide clear next steps or actionable information?",
                        min_score=1,
                        max_score=5,
                        rubric={
                            1: "No actionable information or next steps",
                            2: "Minimal guidance on next steps, lacks specificity",
                            3: "Some actionable information, but could be clearer",
                            4: "Good actionable information with clear next steps",
                            5: "Excellent guidance with specific, comprehensive next steps"
                        }
                    ),
                ]
            )
        elif task_type == "risk_assessment":
            return cls(
                id=f"risk_assessment_eval_{datetime.datetime.now().strftime('%Y%m%d')}",
                title="Risk Assessment Evaluation",
                description="Evaluate the quality of the risk assessment report generated by the LLM.",
                criteria=[
                    HumanEvaluationCriteria(
                        name="risk_identification",
                        description="Does the assessment identify all relevant risks?",
                        min_score=1,
                        max_score=5,
                        rubric={
                            1: "Misses most key risks, major omissions",
                            2: "Identifies few risks, significant omissions",
                            3: "Identifies main risks, but misses some important ones",
                            4: "Identifies most relevant risks with minor omissions",
                            5: "Comprehensively identifies all relevant risks"
                        }
                    ),
                    HumanEvaluationCriteria(
                        name="analysis_quality",
                        description="Is the risk analysis thorough and well-reasoned?",
                        min_score=1,
                        max_score=5,
                        rubric={
                            1: "Very superficial analysis, lacks depth",
                            2: "Basic analysis with limited reasoning",
                            3: "Moderate analysis with some reasoning",
                            4: "Good analysis with solid reasoning",
                            5: "Excellent, comprehensive analysis with strong reasoning"
                        }
                    ),
                    HumanEvaluationCriteria(
                        name="mitigation_strategies",
                        description="Are the proposed mitigation strategies appropriate and effective?",
                        min_score=1,
                        max_score=5,
                        rubric={
                            1: "No effective mitigation strategies",
                            2: "Few helpful mitigation strategies",
                            3: "Some useful strategies, but incomplete",
                            4: "Good strategies that address most risks",
                            5: "Excellent, comprehensive mitigation strategies"
                        }
                    ),
                    HumanEvaluationCriteria(
                        name="practicality",
                        description="Are the assessment and recommendations practical and implementable?",
                        min_score=1,
                        max_score=5,
                        rubric={
                            1: "Not practical or implementable",
                            2: "Low practicality, difficult to implement",
                            3: "Moderately practical, some implementation challenges",
                            4: "Mostly practical with good implementability",
                            5: "Highly practical and readily implementable"
                        }
                    ),
                ]
            )
        else:
            # Generic evaluation form for other tasks
            return cls(
                id=f"generic_eval_{datetime.datetime.now().strftime('%Y%m%d')}",
                title="LLM Output Evaluation",
                description="Evaluate the quality of the content generated by the LLM.",
                criteria=[
                    HumanEvaluationCriteria(
                        name="quality",
                        description="Overall quality of the generated content",
                        min_score=1,
                        max_score=5,
                        rubric={
                            1: "Very poor quality",
                            2: "Below average quality",
                            3: "Average quality",
                            4: "Good quality",
                            5: "Excellent quality"
                        }
                    ),
                    HumanEvaluationCriteria(
                        name="usefulness",
                        description="Usefulness of the generated content for the intended purpose",
                        min_score=1,
                        max_score=5,
                        rubric={
                            1: "Not useful at all",
                            2: "Minimally useful",
                            3: "Moderately useful",
                            4: "Very useful",
                            5: "Extremely useful"
                        }
                    ),
                ]
            )


@dataclass
class HumanEvaluationSubmission:
    """Class representing a single human evaluation submission."""

    id: str = field(
        default_factory=lambda: f"eval_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}")
    form_id: str = ""
    task_type: str = ""
    model_id: str = ""
    evaluator_id: str = ""
    original_text: str = ""
    generated_text: str = ""
    scores: Dict[str, int] = field(default_factory=dict)
    comments: Dict[str, str] = field(default_factory=dict)
    timestamp: str = field(
        default_factory=lambda: datetime.datetime.now().isoformat())

    def get_average_score(self) -> float:
        """Calculate the average score across all criteria."""
        if not self.scores:
            return 0.0
        return sum(self.scores.values()) / len(self.scores)

    def get_scores_summary(self) -> Dict[str, Any]:
        """Get a summary of the evaluation scores."""
        avg_score = self.get_average_score()
        return {
            "average": avg_score,
            "scores": self.scores,
            "task_type": self.task_type,
            "model_id": self.model_id
        }


class HumanEvaluationManager:
    """Manager class for human evaluations."""

    def __init__(self, evaluations_dir: Optional[str] = None):
        """
        Initialize the human evaluation manager.

        Args:
            evaluations_dir: Directory for storing evaluation submissions
        """
        self.evaluations_dir = evaluations_dir or DEFAULT_EVALUATIONS_DIR
        Path(self.evaluations_dir).mkdir(parents=True, exist_ok=True)
        self.forms: Dict[str, HumanEvaluationForm] = {}
        self.submissions: List[HumanEvaluationSubmission] = []
        self._load_submissions()

    def _load_submissions(self):
        """Load existing evaluation submissions from disk."""
        submissions_dir = Path(self.evaluations_dir) / "submissions"
        if not submissions_dir.exists():
            submissions_dir.mkdir(parents=True, exist_ok=True)
            return

        for file_path in submissions_dir.glob("*.json"):
            try:
                with open(file_path, "r") as f:
                    data = json.load(f)
                    submission = HumanEvaluationSubmission(**data)
                    self.submissions.append(submission)
                    logger.info(
                        f"Loaded evaluation submission: {submission.id}")
            except Exception as e:
                logger.error(
                    f"Error loading submission from {file_path}: {str(e)}")

        logger.info(f"Loaded {len(self.submissions)} evaluation submissions")

    def create_form(self, task_type: str) -> HumanEvaluationForm:
        """
        Create an evaluation form for a specific task type.

        Args:
            task_type: Type of insurance task

        Returns:
            An evaluation form
        """
        form = HumanEvaluationForm.create_default_form(task_type)
        self.forms[form.id] = form
        return form

    def save_form(self, form: HumanEvaluationForm):
        """
        Save an evaluation form to disk.

        Args:
            form: The evaluation form to save
        """
        forms_dir = Path(self.evaluations_dir) / "forms"
        forms_dir.mkdir(parents=True, exist_ok=True)

        file_path = forms_dir / f"{form.id}.json"
        try:
            with open(file_path, "w") as f:
                json.dump(asdict(form), f, indent=2)
            logger.info(f"Saved evaluation form to {file_path}")
        except Exception as e:
            logger.error(f"Error saving form to {file_path}: {str(e)}")

    def submit_evaluation(self, submission: HumanEvaluationSubmission) -> bool:
        """
        Submit a human evaluation.

        Args:
            submission: The evaluation submission

        Returns:
            True if successful, False otherwise
        """
        self.submissions.append(submission)

        # Save to disk
        submissions_dir = Path(self.evaluations_dir) / "submissions"
        submissions_dir.mkdir(parents=True, exist_ok=True)

        file_path = submissions_dir / f"{submission.id}.json"
        try:
            with open(file_path, "w") as f:
                json.dump(asdict(submission), f, indent=2)
            logger.info(f"Saved evaluation submission to {file_path}")
            return True
        except Exception as e:
            logger.error(f"Error saving submission to {file_path}: {str(e)}")
            return False

    def get_submissions(
        self,
        task_type: Optional[str] = None,
        model_id: Optional[str] = None,
        form_id: Optional[str] = None
    ) -> List[HumanEvaluationSubmission]:
        """
        Get evaluation submissions with optional filtering.

        Args:
            task_type: Filter by task type
            model_id: Filter by model ID
            form_id: Filter by form ID

        Returns:
            List of matching evaluation submissions
        """
        filtered = self.submissions

        if task_type:
            filtered = [s for s in filtered if s.task_type == task_type]

        if model_id:
            filtered = [s for s in filtered if s.model_id == model_id]

        if form_id:
            filtered = [s for s in filtered if s.form_id == form_id]

        return filtered

    def get_aggregate_scores(
        self,
        task_type: Optional[str] = None,
        model_id: Optional[str] = None
    ) -> Dict[str, Dict[str, float]]:
        """
        Get aggregate scores for evaluations with optional filtering.

        Args:
            task_type: Filter by task type
            model_id: Filter by model ID

        Returns:
            Dictionary with aggregate scores by criteria
        """
        submissions = self.get_submissions(
            task_type=task_type, model_id=model_id)

        if not submissions:
            return {}

        # Group by criteria
        criteria_scores: Dict[str, List[int]] = {}

        for submission in submissions:
            for criterion, score in submission.scores.items():
                if criterion not in criteria_scores:
                    criteria_scores[criterion] = []
                criteria_scores[criterion].append(score)

        # Calculate averages
        aggregates = {
            "overall": {
                "average": sum(s.get_average_score() for s in submissions) / len(submissions),
                "count": len(submissions)
            }
        }

        for criterion, scores in criteria_scores.items():
            aggregates[criterion] = {
                "average": sum(scores) / len(scores),
                "count": len(scores)
            }

        return aggregates

    def export_to_csv(self, output_path: str, task_type: Optional[str] = None) -> bool:
        """
        Export evaluation submissions to CSV.

        Args:
            output_path: Path to save the CSV file
            task_type: Filter by task type

        Returns:
            True if successful, False otherwise
        """
        submissions = self.get_submissions(task_type=task_type)

        if not submissions:
            logger.warning("No submissions to export")
            return False

        # Collect all possible criteria
        all_criteria = set()
        for submission in submissions:
            all_criteria.update(submission.scores.keys())

        try:
            with open(output_path, "w", newline="") as csvfile:
                fieldnames = [
                    "id", "form_id", "task_type", "model_id", "evaluator_id",
                    "timestamp", "average_score"
                ] + [f"score_{c}" for c in sorted(all_criteria)] + ["comments"]

                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()

                for submission in submissions:
                    row = {
                        "id": submission.id,
                        "form_id": submission.form_id,
                        "task_type": submission.task_type,
                        "model_id": submission.model_id,
                        "evaluator_id": submission.evaluator_id,
                        "timestamp": submission.timestamp,
                        "average_score": submission.get_average_score(),
                    }

                    # Add individual scores
                    for criterion in all_criteria:
                        row[f"score_{criterion}"] = submission.scores.get(
                            criterion, "")

                    # Combine comments
                    row["comments"] = "; ".join(
                        f"{k}: {v}" for k, v in submission.comments.items())

                    writer.writerow(row)

            logger.info(
                f"Exported {len(submissions)} submissions to {output_path}")
            return True

        except Exception as e:
            logger.error(f"Error exporting to CSV {output_path}: {str(e)}")
            return False


def get_human_evaluation_manager() -> HumanEvaluationManager:
    """Get or create a global human evaluation manager instance."""
    return HumanEvaluationManager()
