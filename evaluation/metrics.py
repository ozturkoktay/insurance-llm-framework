"""
Evaluation metrics for insurance domain LLM outputs.

This module provides metrics and utilities for evaluating the quality of
LLM-generated content for insurance tasks.
"""

import logging
from typing import Dict, List, Optional, Any, Union, Callable
import numpy as np
from dataclasses import dataclass

import nltk
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt', quiet=True)

try:
    nltk.data.find('stopwords')
except LookupError:
    nltk.download('stopwords', quiet=True)

from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from rouge_score import rouge_scorer
import sacrebleu
from evaluate import load as load_metric

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

@dataclass
class EvaluationResult:
    """Class for storing evaluation results for an LLM output."""

    metric_name: str
    score: float
    max_score: float
    details: Optional[Dict[str, Any]] = None

    def as_dict(self) -> Dict[str, Any]:
        """Convert evaluation result to a dictionary."""
        return {
            "metric_name": self.metric_name,
            "score": self.score,
            "max_score": self.max_score,
            "normalized_score": self.score / self.max_score if self.max_score > 0 else 0,
            "details": self.details
        }

class EvaluationMetric:
    """Base class for evaluation metrics."""

    def __init__(self, name: str, description: str, max_score: float = 1.0):
        """
        Initialize an evaluation metric.

        Args:
            name: Name of the metric
            description: Description of what the metric measures
            max_score: Maximum possible score for the metric
        """
        self.name = name
        self.description = description
        self.max_score = max_score

    def evaluate(
        self,
        generated_text: str,
        reference_text: Optional[str] = None,
        context: Optional[Dict[str, Any]] = None
    ) -> EvaluationResult:
        """
        Evaluate the generated text.

        Args:
            generated_text: Text generated by the model
            reference_text: Ground truth reference text
            context: Additional context for evaluation

        Returns:
            An EvaluationResult object
        """
        raise NotImplementedError("Subclasses must implement this method")

class ROUGEMetric(EvaluationMetric):
    """ROUGE metric for evaluating text summarization quality."""

    def __init__(self, rouge_types: List[str] = None):
        """
        Initialize the ROUGE metric.

        Args:
            rouge_types: ROUGE types to compute
        """
        super().__init__(
            name="rouge",
            description="Measures overlap between generated and reference texts"
        )
        self.rouge_types = rouge_types or ['rouge1', 'rouge2', 'rougeL']
        self.scorer = rouge_scorer.RougeScorer(
            self.rouge_types, use_stemmer=True)

    def evaluate(
        self,
        generated_text: str,
        reference_text: Optional[str] = None,
        context: Optional[Dict[str, Any]] = None
    ) -> EvaluationResult:
        """
        Evaluate text using ROUGE metrics.

        Args:
            generated_text: Text generated by the model
            reference_text: Ground truth reference text
            context: Additional context for evaluation

        Returns:
            ROUGE evaluation result
        """
        if not reference_text:
            raise ValueError("Reference text is required for ROUGE evaluation")

        scores = self.scorer.score(reference_text, generated_text)

        avg_f1 = np.mean(
            [scores[rouge_type].fmeasure for rouge_type in self.rouge_types])

        details = {
            rouge_type: {
                "precision": scores[rouge_type].precision,
                "recall": scores[rouge_type].recall,
                "f1": scores[rouge_type].fmeasure
            }
            for rouge_type in self.rouge_types
        }

        return EvaluationResult(
            metric_name=self.name,
            score=avg_f1,
            max_score=1.0,
            details=details
        )

class BLEUMetric(EvaluationMetric):
    """BLEU metric for evaluating text generation quality."""

    def __init__(self):
        """Initialize the BLEU metric."""
        super().__init__(
            name="bleu",
            description="Measures precision of n-grams between generated and reference texts"
        )

    def evaluate(
        self,
        generated_text: str,
        reference_text: Optional[str] = None,
        context: Optional[Dict[str, Any]] = None
    ) -> EvaluationResult:
        """
        Evaluate text using BLEU metric.

        Args:
            generated_text: Text generated by the model
            reference_text: Ground truth reference text
            context: Additional context for evaluation

        Returns:
            BLEU evaluation result
        """
        if not reference_text:
            raise ValueError("Reference text is required for BLEU evaluation")

        bleu = sacrebleu.corpus_bleu([generated_text], [[reference_text]])

        return EvaluationResult(
            metric_name=self.name,
            score=bleu.score / 100.0,  # Normalize to [0, 1]
            max_score=1.0,
            details={
                "raw_score": bleu.score,
                "precisions": bleu.precisions
            }
        )

class RelevanceMetric(EvaluationMetric):
    """Metric for evaluating relevance of insurance text to the context."""

    def __init__(self, keyword_weights: Optional[Dict[str, float]] = None):
        """
        Initialize the relevance metric.

        Args:
            keyword_weights: Dictionary mapping keywords to their importance weights
        """
        super().__init__(
            name="relevance",
            description="Measures relevance of generated text to insurance context"
        )
        self.keyword_weights = keyword_weights or {}
        self.stop_words = set(stopwords.words('english'))

    def evaluate(
        self,
        generated_text: str,
        reference_text: Optional[str] = None,
        context: Optional[Dict[str, Any]] = None
    ) -> EvaluationResult:
        """
        Evaluate relevance of generated text to insurance context.

        Args:
            generated_text: Text generated by the model
            reference_text: Reference text (optional)
            context: Additional context including keywords to check

        Returns:
            Relevance evaluation result
        """
        context = context or {}

        keywords = context.get('keywords', [])
        if not keywords and reference_text:
            keywords = self._extract_keywords(reference_text)

        if not keywords:
            logger.warning("No keywords available for relevance evaluation")
            return EvaluationResult(
                metric_name=self.name,
                score=0.0,
                max_score=1.0,
                details={"error": "No keywords available"}
            )

        tokens = word_tokenize(generated_text.lower())
        tokens = [t for t in tokens if t.isalnum() and t not in self.stop_words]

        keyword_matched = 0
        keyword_details = {}

        for keyword in keywords:
            weight = self.keyword_weights.get(keyword, 1.0)
            occurrences = sum(
                1 for token in tokens if token == keyword.lower())

            if occurrences > 0:
                keyword_matched += weight
                keyword_details[keyword] = {
                    "occurrences": occurrences,
                    "weight": weight
                }

        total_weight = sum(self.keyword_weights.get(k, 1.0) for k in keywords)
        relevance_score = keyword_matched / total_weight if total_weight > 0 else 0

        return EvaluationResult(
            metric_name=self.name,
            score=relevance_score,
            max_score=1.0,
            details={
                "keywords_matched": len(keyword_details),
                "total_keywords": len(keywords),
                "keyword_details": keyword_details
            }
        )

    def _extract_keywords(self, text: str) -> List[str]:
        """Extract keywords from text."""
        tokens = word_tokenize(text.lower())
        tokens = [t for t in tokens if t.isalnum() and t not in self.stop_words]

        freq_dist = nltk.FreqDist(tokens)
        return [word for word, freq in freq_dist.most_common(10)]

class ContentCompletenessMetric(EvaluationMetric):
    """Metric for evaluating completeness of insurance information in generated text."""

    def __init__(self, required_sections: Optional[List[str]] = None):
        """
        Initialize the content completeness metric.

        Args:
            required_sections: List of required sections/topics to check for
        """
        super().__init__(
            name="completeness",
            description="Measures completeness of insurance information in generated text"
        )
        self.required_sections = required_sections or []

    def evaluate(
        self,
        generated_text: str,
        reference_text: Optional[str] = None,
        context: Optional[Dict[str, Any]] = None
    ) -> EvaluationResult:
        """
        Evaluate completeness of insurance information in generated text.

        Args:
            generated_text: Text generated by the model
            reference_text: Reference text (optional)
            context: Additional context including required_sections

        Returns:
            Completeness evaluation result
        """
        context = context or {}
        required_sections = context.get(
            'required_sections', self.required_sections)

        if not required_sections:
            logger.warning(
                "No required sections specified for completeness evaluation")
            return EvaluationResult(
                metric_name=self.name,
                score=0.0,
                max_score=1.0,
                details={"error": "No required sections specified"}
            )

        section_present = {}
        for section in required_sections:

            present = section.lower() in generated_text.lower()
            section_present[section] = present

        completeness_score = sum(
            1 for present in section_present.values() if present) / len(required_sections)

        return EvaluationResult(
            metric_name=self.name,
            score=completeness_score,
            max_score=1.0,
            details={
                "sections_present": section_present,
                "total_required": len(required_sections),
                "sections_found": sum(1 for present in section_present.values() if present)
            }
        )

class ComplexityMetric(EvaluationMetric):
    """Metric for evaluating text complexity and readability."""

    def __init__(self):
        """Initialize the complexity metric."""
        super().__init__(
            name="complexity",
            description="Measures text complexity and readability"
        )

    def evaluate(
        self,
        generated_text: str,
        reference_text: Optional[str] = None,
        context: Optional[Dict[str, Any]] = None
    ) -> EvaluationResult:
        """
        Evaluate text complexity and readability.

        Args:
            generated_text: Text generated by the model
            reference_text: Reference text (optional)
            context: Additional context (optional)

        Returns:
            Complexity evaluation result
        """

        sentences = nltk.sent_tokenize(generated_text)
        words = word_tokenize(generated_text)

        avg_sentence_length = len(words) / len(sentences) if sentences else 0
        long_words = [w for w in words if len(w) > 6]
        long_word_ratio = len(long_words) / len(words) if words else 0

        total_syllables = sum(self._count_syllables(word) for word in words)
        flesch_reading_ease = 206.835 - \
            (1.015 * avg_sentence_length) - \
            (84.6 * (total_syllables / len(words))) if words else 0

        normalized_ease = max(0, min(flesch_reading_ease, 100)) / 100

        optimal_range_distance = abs(normalized_ease * 100 - 50) / 50

        complexity_score = 1 - optimal_range_distance

        return EvaluationResult(
            metric_name=self.name,
            score=complexity_score,
            max_score=1.0,
            details={
                "avg_sentence_length": avg_sentence_length,
                "long_word_ratio": long_word_ratio,
                "flesch_reading_ease": flesch_reading_ease,
                "sentence_count": len(sentences),
                "word_count": len(words)
            }
        )

    def _count_syllables(self, word: str) -> int:
        """
        Count the number of syllables in a word.
        Simplified approach using vowel groups.
        """
        word = word.lower()
        if len(word) <= 3:
            return 1

        vowels = "aeiouy"
        count = 0
        prev_is_vowel = False

        for char in word:
            is_vowel = char in vowels
            if is_vowel and not prev_is_vowel:
                count += 1
            prev_is_vowel = is_vowel

        if word.endswith('e'):
            count -= 1
        if word.endswith('le') and len(word) > 2 and word[-3] not in vowels:
            count += 1
        if count == 0:
            count = 1

        return count

class ComplianceMetric(EvaluationMetric):
    """Metric for evaluating compliance with insurance regulations and terminology."""

    def __init__(
        self,
        required_phrases: Optional[List[str]] = None,
        prohibited_phrases: Optional[List[str]] = None
    ):
        """
        Initialize the compliance metric.

        Args:
            required_phrases: Phrases that should be included
            prohibited_phrases: Phrases that should be avoided
        """
        super().__init__(
            name="compliance",
            description="Measures compliance with insurance standards and regulations"
        )
        self.required_phrases = required_phrases or []
        self.prohibited_phrases = prohibited_phrases or []

    def evaluate(
        self,
        generated_text: str,
        reference_text: Optional[str] = None,
        context: Optional[Dict[str, Any]] = None
    ) -> EvaluationResult:
        """
        Evaluate compliance with insurance standards and regulations.

        Args:
            generated_text: Text generated by the model
            reference_text: Reference text (optional)
            context: Additional context with required/prohibited phrases

        Returns:
            Compliance evaluation result
        """
        context = context or {}
        required_phrases = context.get(
            'required_phrases', self.required_phrases)
        prohibited_phrases = context.get(
            'prohibited_phrases', self.prohibited_phrases)

        generated_lower = generated_text.lower()

        required_present = {}
        for phrase in required_phrases:
            required_present[phrase] = phrase.lower() in generated_lower

        prohibited_present = {}
        for phrase in prohibited_phrases:
            prohibited_present[phrase] = phrase.lower() in generated_lower

        required_score = sum(1 for present in required_present.values(
        ) if present) / max(1, len(required_phrases))
        prohibited_score = 1 - (sum(1 for present in prohibited_present.values()
                                if present) / max(1, len(prohibited_phrases)))

        compliance_score = 0.7 * required_score + 0.3 * \
            prohibited_score if (
                required_phrases or prohibited_phrases) else 0.5

        return EvaluationResult(
            metric_name=self.name,
            score=compliance_score,
            max_score=1.0,
            details={
                "required_phrases": required_present,
                "prohibited_phrases": prohibited_present,
                "required_score": required_score,
                "prohibited_score": prohibited_score
            }
        )

class EvaluationMetrics:
    """Manager class for evaluation metrics."""

    def __init__(self):
        """Initialize the evaluation metrics manager."""
        self.metrics: Dict[str, EvaluationMetric] = {}
        self._register_default_metrics()

    def _register_default_metrics(self):
        """Register default evaluation metrics."""
        self.register_metric(ROUGEMetric())
        self.register_metric(BLEUMetric())
        self.register_metric(RelevanceMetric())
        self.register_metric(ContentCompletenessMetric())
        self.register_metric(ComplexityMetric())
        self.register_metric(ComplianceMetric())

    def register_metric(self, metric: EvaluationMetric):
        """
        Register an evaluation metric.

        Args:
            metric: The metric to register
        """
        self.metrics[metric.name] = metric
        logger.info(f"Registered metric: {metric.name}")

    def get_metric(self, name: str) -> Optional[EvaluationMetric]:
        """
        Get a metric by name.

        Args:
            name: Name of the metric

        Returns:
            The metric or None if not found
        """
        return self.metrics.get(name)

    def list_metrics(self) -> List[Dict[str, str]]:
        """
        List all available metrics.

        Returns:
            List of dictionaries with metric information
        """
        return [
            {"name": metric.name, "description": metric.description}
            for metric in self.metrics.values()
        ]

    def evaluate(
        self,
        generated_text: str,
        metric_names: Optional[List[str]] = None,
        reference_text: Optional[str] = None,
        context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, EvaluationResult]:
        """
        Evaluate generated text using specified metrics.

        Args:
            generated_text: Text generated by the model
            metric_names: Names of metrics to use (all if None)
            reference_text: Reference text for comparison
            context: Additional context for evaluation

        Returns:
            Dictionary mapping metric names to evaluation results
        """
        results = {}

        metric_names = metric_names or list(self.metrics.keys())

        for name in metric_names:
            metric = self.get_metric(name)
            if metric:
                try:
                    result = metric.evaluate(
                        generated_text, reference_text, context)
                    results[name] = result
                except Exception as e:
                    logger.error(
                        f"Error evaluating with metric {name}: {str(e)}")
                    results[name] = EvaluationResult(
                        metric_name=name,
                        score=0.0,
                        max_score=1.0,
                        details={"error": str(e)}
                    )
            else:
                logger.warning(f"Metric not found: {name}")

        return results

    def get_aggregate_score(
        self,
        results: Dict[str, EvaluationResult],
        weights: Optional[Dict[str, float]] = None
    ) -> float:
        """
        Calculate an aggregate score from individual metric results.

        Args:
            results: Dictionary mapping metric names to evaluation results
            weights: Dictionary mapping metric names to their weights

        Returns:
            Weighted average score
        """
        weights = weights or {name: 1.0 for name in results.keys()}

        total_weight = 0.0
        weighted_sum = 0.0

        for name, result in results.items():
            weight = weights.get(name, 0.0)
            total_weight += weight
            weighted_sum += weight * (result.score / result.max_score)

        return weighted_sum / total_weight if total_weight > 0 else 0.0

def get_metrics_manager() -> EvaluationMetrics:
    """Get or create a global metrics manager instance."""
    return EvaluationMetrics()
